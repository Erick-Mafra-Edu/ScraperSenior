#!/usr/bin/env python3
"""
Orchestrator: Scrape ‚Üí Index ‚Üí Export
Complete workflow for scraping Senior documentation and indexing with Meilisearch.
"""

import subprocess
import time
import requests
import json
import sys
from pathlib import Path


def run_command(cmd, description):
    """Run a shell command."""
    print(f"\n{'='*70}")
    print(f"‚ñ∂Ô∏è  {description}")
    print(f"{'='*70}\n")
    
    result = subprocess.run(cmd, shell=True)
    
    if result.returncode != 0:
        print(f"\n‚ùå Error: {description} failed")
        return False
    
    return True


def wait_for_api(max_retries=10):
    """Wait for API to be ready."""
    print("\n‚è≥ Waiting for API to be ready...")
    
    for i in range(max_retries):
        try:
            response = requests.get("http://localhost:5000/health", timeout=5)
            if response.status_code == 200:
                print("‚úÖ API is ready\n")
                return True
        except:
            print(f"  Retry {i+1}/{max_retries}...")
            time.sleep(2)
    
    return False


def check_api():
    """Check if API is running."""
    try:
        response = requests.get("http://localhost:5000/health", timeout=5)
        return response.status_code == 200
    except:
        return False


def scrape_documentation(url, max_pages):
    """Scrape documentation."""
    print(f"\n{'='*70}")
    print(f"üï∑Ô∏è  SCRAPING DOCUMENTATION")
    print(f"{'='*70}")
    print(f"URL: {url}")
    print(f"Max Pages: {max_pages}\n")
    
    cmd = f"python scraper_senior.py '{url}' {max_pages}"
    
    if not run_command(cmd, f"Scraping {url} (max {max_pages} pages)"):
        return False
    
    # Check if any files were created
    doc_dir = Path("documentacao")
    if doc_dir.exists():
        pages = list(doc_dir.glob("*/metadata.json"))
        print(f"\n‚úÖ Found {len(pages)} pages to index")
        return True
    
    return False


def index_documents():
    """Index documents via API."""
    print(f"\n{'='*70}")
    print(f"üìë INDEXING DOCUMENTS")
    print(f"{'='*70}\n")
    
    try:
        response = requests.post("http://localhost:5000/index", timeout=60)
        
        if response.status_code == 200:
            data = response.json()
            indexed = data.get("documents_indexed", 0)
            print(f"‚úÖ Successfully indexed {indexed} documents\n")
            return True
        else:
            print(f"‚ùå Indexing failed: {response.status_code}")
            print(response.text)
            return False
    
    except Exception as e:
        print(f"‚ùå Error indexing: {e}")
        return False


def get_stats():
    """Get index statistics."""
    try:
        response = requests.get("http://localhost:5000/stats", timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            return data
    
    except Exception as e:
        print(f"Error getting stats: {e}")
    
    return None


def verify_indexing():
    """Verify that documents were indexed."""
    print(f"\n{'='*70}")
    print(f"‚úÖ VERIFYING INDEX")
    print(f"{'='*70}\n")
    
    stats = get_stats()
    
    if not stats:
        print("‚ùå Could not retrieve stats")
        return False
    
    doc_count = stats.get("documents_count", 0)
    
    print(f"üìä Index Statistics:")
    print(f"   ‚Ä¢ Total Documents: {doc_count}")
    print(f"   ‚Ä¢ Fields: {len(stats.get('field_distribution', {}))}")
    
    if doc_count > 0:
        print(f"\n‚úÖ Index contains {doc_count} documents")
        return True
    else:
        print("\n‚ö†Ô∏è  No documents indexed")
        return False


def test_search():
    """Test search functionality."""
    print(f"\n{'='*70}")
    print(f"üîç TESTING SEARCH")
    print(f"{'='*70}\n")
    
    test_queries = ["python", "api", "tecnologia"]
    
    for query in test_queries:
        try:
            response = requests.get(
                "http://localhost:5000/search",
                params={"q": query, "limit": 3},
                timeout=10
            )
            
            if response.status_code == 200:
                data = response.json()
                hits = data.get("total_hits", 0)
                print(f"  üîç '{query}': {hits} results")
            else:
                print(f"  ‚ùå Search failed for '{query}'")
        
        except Exception as e:
            print(f"  ‚ùå Error searching '{query}': {e}")


def export_data():
    """Export indexed data."""
    print(f"\n{'='*70}")
    print(f"üíæ EXPORTING DATA FOR AI")
    print(f"{'='*70}\n")
    
    formats = ["jsonl", "csv"]
    
    for fmt in formats:
        try:
            response = requests.get(
                "http://localhost:5000/export",
                params={"format": fmt},
                timeout=60
            )
            
            if response.status_code == 200:
                filename = f"export_data.{fmt}"
                with open(filename, "w", encoding="utf-8") as f:
                    f.write(response.text)
                
                size = Path(filename).stat().st_size
                print(f"‚úÖ Exported {fmt.upper()}: {filename} ({size/1024:.1f} KB)")
            else:
                print(f"‚ùå Export failed for {fmt}")
        
        except Exception as e:
            print(f"‚ùå Error exporting {fmt}: {e}")


def main():
    """Main workflow."""
    print("\n" + "="*70)
    print("üöÄ SENIOR DOCUMENTATION PIPELINE")
    print("="*70)
    print("Workflow: Scrape ‚Üí Index ‚Üí Verify ‚Üí Export")
    print("="*70)
    
    # Parse arguments
    url = sys.argv[1] if len(sys.argv) > 1 else "https://documentacao.senior.com.br"
    max_pages = int(sys.argv[2]) if len(sys.argv) > 2 else 50
    
    # Check if API is running
    print("\n1Ô∏è‚É£  Checking API status...")
    if not check_api():
        print("‚ùå API is not running!")
        print("Start it with: docker-compose up -d")
        return
    print("‚úÖ API is running\n")
    
    # Step 1: Scrape
    print("\n2Ô∏è‚É£  Starting scraper...")
    if not scrape_documentation(url, max_pages):
        print("‚ùå Scraping failed")
        return
    
    # Wait a moment
    time.sleep(2)
    
    # Step 2: Index
    print("\n3Ô∏è‚É£  Indexing documents...")
    if not index_documents():
        print("‚ùå Indexing failed")
        return
    
    # Wait for indexing to complete
    time.sleep(3)
    
    # Step 3: Verify
    print("\n4Ô∏è‚É£  Verifying index...")
    if not verify_indexing():
        print("‚ö†Ô∏è  Verification showed no documents")
    
    # Step 4: Test Search
    print("\n5Ô∏è‚É£  Testing search...")
    test_search()
    
    # Step 5: Export
    print("\n6Ô∏è‚É£  Exporting data...")
    export_data()
    
    # Summary
    print("\n" + "="*70)
    print("‚úÖ PIPELINE COMPLETE!")
    print("="*70)
    print("\nWhat's next:")
    print("  ‚Ä¢ View Meilisearch UI: http://localhost:7700")
    print("  ‚Ä¢ Search API: http://localhost:5000/search?q=<query>")
    print("  ‚Ä¢ Export more formats: curl 'http://localhost:5000/export?format=json'")
    print("  ‚Ä¢ Use export files for AI training")
    print("="*70 + "\n")


if __name__ == "__main__":
    main()
