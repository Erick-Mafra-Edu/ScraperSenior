# Exemplos Avan√ßados - Scraper Modular

## 1. Scraper para Site com Abas Din√¢micas

```json
{
  "scraper": {
    "base_url": "https://documentacao.senior.com.br",
    "max_pages": 100
  },
  "javascript_handling": {
    "enable_js_interaction": true,
    "execute_scripts": [
      {
        "name": "remove_modals",
        "script": "document.querySelectorAll('.modal, [role=\"dialog\"]').forEach(el => el.remove())"
      }
    ],
    "click_and_wait": [
      {
        "selector": "[role='tab'], .tab-header, [class*='tab']",
        "wait_ms": 1500,
        "description": "Clica em cada aba e aguarda conte√∫do carregar",
        "detect_change": {
          "monitor_selector": ".tab-content, [role='tabpanel'], [aria-selected='true']",
          "check_visibility": true,
          "max_retries": 3
        }
      }
    ]
  }
}
```

## 2. Scraper para Documenta√ß√£o com Navega√ß√£o em √Årvore

```json
{
  "javascript_handling": {
    "enable_js_interaction": true,
    "click_and_wait": [
      {
        "selector": ".toc-item, .tree-node, [class*='expandable']",
        "wait_ms": 800,
        "description": "Expande cada item da √°rvore de navega√ß√£o",
        "detect_change": {
          "monitor_selector": ".tree-expanded, [aria-expanded='true'], .visible-children",
          "check_content_change": true
        }
      }
    ],
    "execute_scripts": [
      {
        "name": "expand_all_trees",
        "script": "document.querySelectorAll('[aria-expanded=\"false\"]').forEach(el => el.click())"
      }
    ]
  },
  "cleanup": {
    "garbage_sequences": [
      {
        "pattern": "^(Voltar|P√°gina anterior|Pr√≥xima)$",
        "action": "remove"
      }
    ]
  }
}
```

## 3. Scraper para Site com Muitos An√∫ncios e Rastreamento

```json
{
  "cleanup": {
    "garbage_patterns": [
      "<!--.*?-->",
      "<script.*?</script>",
      "<style.*?</style>",
      "<noscript.*?</noscript>"
    ],
    "garbage_sequences": [
      {
        "pattern": "(google_ad_|adsbygoogle|doubleclick|ads\\.google)",
        "action": "remove",
        "description": "Remove tags de an√∫ncios Google"
      },
      {
        "pattern": "(facebook\\.com/tr|fbq\\(|gtag\\(|ga\\(|_trackPageview)",
        "action": "remove",
        "description": "Remove tracking de Facebook, Google Analytics"
      },
      {
        "pattern": "(cookie.*?consent|gdpr.*?banner|privacy.*?notice)",
        "action": "remove",
        "description": "Remove banners de cookie/GDPR"
      },
      {
        "pattern": "\\[.{0,5}(ad|announcement|promo).*?\\]",
        "action": "remove"
      }
    ]
  },
  "javascript_handling": {
    "execute_scripts": [
      {
        "name": "remove_ads",
        "script": "document.querySelectorAll('[class*=\"ad\"], [class*=\"advertisement\"], [id*=\"ad\"], [id*=\"advertisement\"]').forEach(el => el.remove())"
      },
      {
        "name": "remove_tracking",
        "script": "document.querySelectorAll('img[src*=\"doubleclick\"], img[src*=\"facebook\"], img[src*=\"google\"]').forEach(el => el.remove())"
      },
      {
        "name": "remove_popup",
        "script": "document.querySelectorAll('.popup, .modal, .overlay, [class*=\"modal\"]').forEach(el => el.remove())"
      }
    ]
  }
}
```

## 4. Scraper para Site com Pagina√ß√£o

```json
{
  "scraper": {
    "max_pages": 500
  },
  "javascript_handling": {
    "click_and_wait": [
      {
        "selector": "a[rel='next'], .next-page, [aria-label*='Next'], .pagination a:last-child",
        "wait_ms": 2000,
        "description": "Clica em 'Pr√≥xima p√°gina' para paginar",
        "detect_change": {
          "monitor_selector": ".post, article, [data-post-id]",
          "check_content_change": true
        }
      }
    ]
  },
  "cleanup": {
    "garbage_sequences": [
      {
        "pattern": "^(Pr√≥xima p√°gina|P√°gina anterior|Ver mais|Carregando)$",
        "action": "remove"
      }
    ]
  }
}
```

## 5. Scraper para Documenta√ß√£o com Exemplos de C√≥digo

```json
{
  "extraction": {
    "extract_code_blocks": true,
    "max_content_length": 200000
  },
  "cleanup": {
    "garbage_sequences": [
      {
        "pattern": "^\\s*Copy\\s*$",
        "action": "remove",
        "description": "Remove bot√£o 'Copy' de blocos de c√≥digo"
      },
      {
        "pattern": "^\\s*Show lines\\s*$",
        "action": "remove"
      }
    ]
  },
  "selectors": {
    "content": [
      "#main-content",
      ".documentation",
      "article",
      "[data-content-area]"
    ],
    "skip": [
      "script",
      "style",
      ".hidden",
      "[aria-hidden='true']"
    ]
  }
}
```

## 6. Scraper para Wiki/Enciclop√©dia

```json
{
  "javascript_handling": {
    "enable_js_interaction": true,
    "click_and_wait": [
      {
        "selector": "a[href*='#']",
        "wait_ms": 500,
        "description": "Clica em links √¢ncora para carregar se√ß√µes"
      }
    ],
    "execute_scripts": [
      {
        "name": "remove_references_popup",
        "script": "document.querySelectorAll('.reference-popup, .citation-popup').forEach(el => el.remove())"
      }
    ]
  },
  "cleanup": {
    "garbage_sequences": [
      {
        "pattern": "\\[\\s*citation needed\\s*\\]",
        "action": "remove"
      },
      {
        "pattern": "\\[edit\\]",
        "action": "remove"
      }
    ]
  },
  "selectors": {
    "content": [
      "#mw-content-text",
      ".mw-parser-output",
      "#page-content",
      "main"
    ]
  }
}
```

## 7. Uso Program√°tico com Customiza√ß√µes Din√¢micas

```python
from src.scraper_modular import ModularScraper, ConfigManager
import json

# Carrega config base
config = ConfigManager("scraper_config.json")

# Customiza dinamicamente
new_config = config.config.copy()
new_config['scraper']['max_pages'] = 200
new_config['extraction']['max_content_length'] = 150000
new_config['cleanup']['garbage_sequences'].append({
    "pattern": r"seu_padr√£o_customizado",
    "action": "remove"
})

# Salva config customizada
with open("config_temp.json", "w") as f:
    json.dump(new_config, f)

# Executa com config customizada
import asyncio

async def run():
    scraper = ModularScraper("config_temp.json")
    await scraper.scrape()

asyncio.run(run())
```

## 8. Scraper com Logging Detalhado

```python
import logging
from src.scraper_modular import ModularScraper

# Configura logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger('scraper')

# Executa com logging
import asyncio

async def run():
    scraper = ModularScraper("scraper_config.json")
    logger.info(f"Iniciando scraper para {scraper.config.get('scraper.base_url')}")
    logger.info(f"M√°x p√°ginas: {scraper.config.get('scraper.max_pages')}")
    await scraper.scrape()

asyncio.run(run())
```

## 9. Scraper com Valida√ß√£o Customizada

```python
from src.scraper_modular import ModularScraper, ConfigManager
import json

class ValidatedScraper(ModularScraper):
    """Scraper com valida√ß√µes customizadas"""
    
    async def _scrape_page(self, page, url: str) -> bool:
        """Override para adicionar valida√ß√µes"""
        # Executa scraping normal
        result = await super()._scrape_page(page, url)
        
        if result and self.documents:
            # Valida √∫ltimo documento
            doc = self.documents[-1]
            
            # Verifica se t√≠tulo n√£o √© vazio
            if not doc['title'] or len(doc['title']) < 5:
                print(f"‚ö†Ô∏è  T√≠tulo muito curto: {url}")
                self.documents.pop()
                return False
            
            # Verifica se tem conte√∫do suficiente
            if len(doc['content']) < 500:
                print(f"‚ö†Ô∏è  Conte√∫do muito curto: {url}")
                self.documents.pop()
                return False
        
        return result

# Usa scraper customizado
import asyncio

async def run():
    scraper = ValidatedScraper("scraper_config.json")
    await scraper.scrape()

asyncio.run(run())
```

## 10. Extra√ß√£o Seletiva por Tipo de P√°gina

```json
{
  "scraper": {
    "base_url": "https://documentacao.senior.com.br",
    "max_pages": 200
  },
  "selectors": {
    "title": [
      "h1",
      ".page-title",
      "[data-document-title]"
    ],
    "content": [
      "#main-content",
      ".documentation-content"
    ]
  },
  "cleanup": {
    "garbage_sequences": [
      {
        "pattern": "\\b(Deprecated|Obsolete|Legacy)\\b",
        "action": "remove",
        "description": "Remove tags de deprecation"
      },
      {
        "pattern": "^\\s*(Vers√£o|Version):\\s*(\\d+\\.)*\\d+.*$",
        "action": "remove",
        "description": "Remove indicadores de vers√£o espec√≠fica"
      }
    ]
  },
  "javascript_handling": {
    "enable_js_interaction": true,
    "click_and_wait": [
      {
        "selector": "[data-expandable='true']",
        "wait_ms": 600,
        "detect_change": {
          "monitor_selector": "[data-expanded='true']",
          "check_visibility": true
        }
      }
    ]
  }
}
```

---

## üìä Monitorar Performance

```python
from src.scraper_modular import ModularScraper
import time

class MonitoredScraper(ModularScraper):
    """Scraper com monitoramento de performance"""
    
    async def _scrape_page(self, page, url: str) -> bool:
        start = time.time()
        result = await super()._scrape_page(page, url)
        duration = time.time() - start
        
        if result:
            doc = self.documents[-1]
            bytes_per_second = doc['metadata']['content_length'] / duration
            print(f"  ‚è±Ô∏è  {duration:.2f}s | {bytes_per_second:.0f} chars/s | {doc['metadata']['content_length']} chars")
        
        return result
```

---

## üîÑ Processamento em Batch

```python
from src.scraper_modular import ModularScraper
import asyncio
import json

async def scrape_multiple_sites():
    """Scrapa m√∫ltiplos sites com diferentes configura√ß√µes"""
    
    sites = [
        ("https://documentacao.senior.com.br", "senior_docs"),
        ("https://help.example.com", "example_help"),
        ("https://docs.example.org", "example_docs")
    ]
    
    for base_url, name in sites:
        config = {
            "scraper": {
                "base_url": base_url,
                "max_pages": 100
            },
            "output": {
                "save_directory": f"docs_{name}"
            }
        }
        
        with open(f"config_{name}.json", "w") as f:
            json.dump(config, f)
        
        scraper = ModularScraper(f"config_{name}.json")
        await scraper.scrape()

asyncio.run(scrape_multiple_sites())
```

---

## üíæ P√≥s-processamento dos Dados

```python
from src.scraper_modular import ModularScraper
import json
from pathlib import Path

class PostProcessingScraper(ModularScraper):
    """Scraper com p√≥s-processamento"""
    
    def _save_documents(self):
        """Override para adicionar p√≥s-processamento"""
        # Executa salvamento normal
        super()._save_documents()
        
        # P√≥s-processamento
        print("\nüìä P√≥s-processamento...")
        
        # Indexa por m√≥dulo
        by_module = {}
        for doc in self.documents:
            module = doc.get('module', 'Unknown')
            if module not in by_module:
                by_module[module] = []
            by_module[module].append(doc)
        
        # Salva √≠ndice
        with open("docs_by_module.json", "w") as f:
            json.dump({m: len(docs) for m, docs in by_module.items()}, f)
        
        # Estat√≠sticas
        print(f"üìà {len(self.documents)} documentos em {len(by_module)} m√≥dulos")
```

---

## üéØ Casos de Uso Espec√≠ficos

### Extrair Apenas Headers
```python
async def extract_headers_only():
    config = ConfigManager("scraper_config.json")
    config.config['extraction']['min_content_length'] = 1
    # Remove conte√∫do sem headers
```

### Extrair Apenas Links
```python
async def extract_links_only():
    # Configure para n√£o extrair conte√∫do
    config = ConfigManager("scraper_config.json")
    config.config['extraction']['extract_links'] = True
```

### Validar Conformidade
```python
def validate_links():
    """Valida se links est√£o vivos"""
    for doc in scraper.documents:
        url = doc['url']
        # Verificar status code
```
