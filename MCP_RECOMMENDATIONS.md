# Recomenda√ß√µes Finais: MCP, Docker e Meilisearch

**Baseado em**: Valida√ß√£o completa realizada em 30 de janeiro de 2026

---

## üéØ Prioridade 1: Cr√≠tico (Implementar ANTES de produ√ß√£o)

### 1.1 Seguran√ßa - API Key do Meilisearch

**Problema**: A chave `meilisearch_master_key_change_me` √© hardcoded

**Solu√ß√£o**:
```yaml
# infra/docker/docker-compose.yml
environment:
  MEILI_MASTER_KEY: ${MEILI_MASTER_KEY}  # Obrigat√≥rio via env
```

**Arquivo**: `.env` (n√£o commitar)
```
MEILI_MASTER_KEY=seu_master_key_super_seguro_aleatorio
MEILISEARCH_KEY=seu_master_key_super_seguro_aleatorio
```

**Implementa√ß√£o**:
```bash
# Gerar chave segura
openssl rand -base64 32

# Criar .env
echo "MEILI_MASTER_KEY=$(openssl rand -base64 32)" > .env
echo "MEILISEARCH_KEY=$(openssl rand -base64 32)" >> .env

# Adicionar ao .gitignore
echo ".env" >> .gitignore
```

**Status**: ‚ö†Ô∏è Recomendado

---

### 1.2 Atualizar Path em mcp_config.json

**Problema**: Path ainda referencia estrutura antiga (`src/mcp_server.py`)

**Solu√ß√£o**:
```json
{
    "mcpServers": {
        "senior-docs": {
            "command": "python",
            "args": ["apps/mcp-server/mcp_server.py"],  // ‚Üê ATUALIZAR
            "cwd": "c:/Users/Digisys/scrapyTest"
        }
    }
}
```

**Status**: üî¥ Cr√≠tico

---

### 1.3 HEALTHCHECK no Dockerfile do Scraper

**Problema**: Dockerfile do Scraper n√£o possui HEALTHCHECK

**Solu√ß√£o**:
```dockerfile
# infra/docker/Dockerfile - Adicionar ao final
HEALTHCHECK --interval=60s --timeout=10s --start-period=10s --retries=3 \
    CMD python -c "import sys; sys.exit(0)" || exit 1
```

**Status**: üü° Importante

---

## üéØ Prioridade 2: Alta (Implementar em 2-4 semanas)

### 2.1 Monitoramento e Logging

**Implementa√ß√£o**: Prometheus + Grafana

```yaml
# infra/docker/docker-compose.yml - Adicionar servi√ßo
prometheus:
  image: prom/prometheus:latest
  volumes:
    - ./prometheus.yml:/etc/prometheus/prometheus.yml
    - prometheus_data:/prometheus
  ports:
    - "9090:9090"
  networks:
    - senior-docs

grafana:
  image: grafana/grafana:latest
  ports:
    - "3000:3000"
  volumes:
    - grafana_data:/var/lib/grafana
  networks:
    - senior-docs
```

**M√©tricas a monitorar**:
- Lat√™ncia de buscas Meilisearch
- Taxa de erro (5xx)
- Uso de mem√≥ria
- Uptime dos servi√ßos

**Status**: üü° Importante

---

### 2.2 Rate Limiting

**Implementa√ß√£o**: Adicionar middleware ao MCP Server

```python
# apps/mcp-server/middleware.py (novo arquivo)
from functools import wraps
from time import time
from collections import defaultdict

class RateLimiter:
    def __init__(self, requests_per_second=100):
        self.requests_per_second = requests_per_second
        self.requests = defaultdict(list)
    
    def is_allowed(self, client_id: str) -> bool:
        now = time()
        # Remover requisi√ß√µes antigas
        self.requests[client_id] = [
            req_time for req_time in self.requests[client_id]
            if now - req_time < 1
        ]
        
        if len(self.requests[client_id]) < self.requests_per_second:
            self.requests[client_id].append(now)
            return True
        return False
```

**Status**: üü° Importante

---

### 2.3 Backup Autom√°tico do √çndice

**Script**: `scripts/indexing/backup_meilisearch.py`

```python
#!/usr/bin/env python3
import meilisearch
import json
from datetime import datetime
from pathlib import Path

def backup_meilisearch_index():
    client = meilisearch.Client("http://localhost:7700", "api_key")
    index = client.index("documentation")
    
    # Exportar documentos
    documents = []
    for doc in index.get_documents({"limit": 10000})["results"]:
        documents.append(doc)
    
    # Salvar backup
    backup_dir = Path("data/backups")
    backup_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().isoformat()
    backup_file = backup_dir / f"meilisearch_backup_{timestamp}.jsonl"
    
    with open(backup_file, 'w', encoding='utf-8') as f:
        for doc in documents:
            f.write(json.dumps(doc, ensure_ascii=False) + '\n')
    
    print(f"‚úì Backup criado: {backup_file}")

if __name__ == "__main__":
    backup_meilisearch_index()
```

**Agendamento**: Cron job di√°rio
```bash
# Adicionar a crontab
0 2 * * * cd /path/to/project && python scripts/indexing/backup_meilisearch.py
```

**Status**: üü° Importante

---

## üéØ Prioridade 3: M√©dia (Implementar em 4-8 semanas)

### 3.1 Cache com Redis

**Docker Service**:
```yaml
redis:
  image: redis:7-alpine
  ports:
    - "6379:6379"
  volumes:
    - redis_data:/data
  networks:
    - senior-docs
  healthcheck:
    test: ["CMD", "redis-cli", "ping"]
    interval: 5s
    timeout: 3s
    retries: 5
```

**MCP Integration**:
```python
import redis

class CachedSearchMCP:
    def __init__(self, redis_host='localhost', redis_port=6379):
        self.redis = redis.Redis(host=redis_host, port=redis_port, decode_responses=True)
    
    def search(self, query: str, module: str = None, limit: int = 5):
        # Gerar cache key
        cache_key = f"search:{query}:{module}:{limit}"
        
        # Verificar cache
        cached = self.redis.get(cache_key)
        if cached:
            return json.loads(cached)
        
        # Se n√£o estiver em cache, fazer busca
        results = self._do_search(query, module, limit)
        
        # Salvar em cache por 1 hora
        self.redis.setex(cache_key, 3600, json.dumps(results))
        
        return results
```

**Status**: üü° M√©dia

---

### 3.2 HTTPS/TLS

**Certificado**: Let's Encrypt via Certbot

```bash
# Instalar Certbot
apt-get install certbot python3-certbot-nginx

# Gerar certificado
certbot certonly --standalone -d seu_dominio.com

# Configurar Nginx como reverse proxy
```

**Nginx Config**:
```nginx
server {
    listen 443 ssl;
    server_name seu_dominio.com;
    
    ssl_certificate /etc/letsencrypt/live/seu_dominio.com/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/seu_dominio.com/privkey.pem;
    
    location / {
        proxy_pass http://localhost:8000;
        proxy_set_header X-Forwarded-For $remote_addr;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

**Status**: üü° M√©dia

---

### 3.3 Replica√ß√£o de √çndices

**Usar Meilisearch Cloud** para backup e failover autom√°tico

Ou **implementar sincroniza√ß√£o manual**:

```python
# Script para sincronizar √≠ndices entre inst√¢ncias
import meilisearch

def sync_indices(master_url, replica_url):
    master = meilisearch.Client(master_url, "api_key_master")
    replica = meilisearch.Client(replica_url, "api_key_replica")
    
    # Exportar √≠ndice do master
    index = master.index("documentation")
    documents = index.get_documents({"limit": 10000})["results"]
    
    # Importar no replica
    replica_index = replica.index("documentation")
    replica_index.add_documents(documents)
    
    print(f"‚úì {len(documents)} documentos sincronizados")
```

**Status**: üü° M√©dia

---

## üéØ Prioridade 4: Baixa (Nice to have)

### 4.1 Dashboard de Administra√ß√£o

**Implementar**: Web UI para:
- Visualizar estat√≠sticas do √≠ndice
- Gerenciar documentos
- Hist√≥rico de buscas
- Performance metrics

**Stack**: FastAPI + React

---

### 4.2 Analytics

**Rastrear**:
- Buscas mais frequentes
- Documentos mais acessados
- Tempo de resposta
- Taxa de cliques

```python
# apps/mcp-server/analytics.py
class SearchAnalytics:
    def __init__(self):
        self.searches = []
    
    def log_search(self, query, results_count, response_time_ms):
        self.searches.append({
            "query": query,
            "results": results_count,
            "response_time": response_time_ms,
            "timestamp": datetime.now()
        })
```

---

### 4.3 Machine Learning (Ranking Personalizado)

**Usar**: LLM para re-ranker de resultados

```python
from langchain.llms import OpenAI

def rerank_results(query, results):
    llm = OpenAI(api_key="...")
    
    prompt = f"""
    Dado a query: "{query}"
    E estes resultados:
    {results}
    
    Reordene por relev√¢ncia. Retorne em JSON.
    """
    
    reranked = llm.predict(prompt)
    return json.loads(reranked)
```

---

## üìã Checklist de Implementa√ß√£o

### Antes de Produ√ß√£o (1-2 semanas)
- [ ] Atualizar mcp_config.json com novo path
- [ ] Configurar vari√°veis de ambiente (.env)
- [ ] Adicionar HEALTHCHECK ao Dockerfile do Scraper
- [ ] Testar em staging environment
- [ ] Revisar logs de seguran√ßa

### Primeira Semana em Produ√ß√£o
- [ ] Monitorar uptime e performance
- [ ] Verificar backup autom√°tico
- [ ] Testar fallback para JSONL
- [ ] Revisar logs de erros

### Pr√≥ximas 2-4 Semanas
- [ ] Implementar Prometheus/Grafana
- [ ] Adicionar rate limiting
- [ ] Configurar backup autom√°tico
- [ ] Documentar runbook de ops

---

## üîß Troubleshooting R√°pido

### Problema: Meilisearch n√£o conecta
```bash
# Verificar sa√∫de
curl http://localhost:7700/health

# Ver logs
docker logs senior-docs-meilisearch

# Reiniciar
docker-compose restart meilisearch
```

### Problema: MCP Server n√£o responde
```bash
# Verificar sa√∫de
curl http://localhost:8000/health

# Ver logs
docker logs senior-docs-mcp-server

# Reiniciar
docker-compose restart mcp-server
```

### Problema: Buscas lentas
```bash
# Verificar performance
curl http://localhost:8000/stats

# Dados podem estar desatualizados
# Reindexar:
python scripts/indexing/reindex_all_docs.py
```

---

## üìû Contato e Suporte

- **Documenta√ß√£o**: `docs/guides/`
- **Relat√≥rio de Valida√ß√£o**: `MCP_VALIDATION_REPORT.md`
- **Resumo Executivo**: `MCP_VALIDATION_EXECUTIVE_SUMMARY.md`
- **Issues**: GitHub Issues (quando dispon√≠vel)

---

## ‚úÖ Conclus√£o

Seguindo estas recomenda√ß√µes, o sistema estar√°:
1. ‚úÖ Seguro (HTTPS, API keys rotativas)
2. ‚úÖ Confi√°vel (Backups, Fallback, Healthchecks)
3. ‚úÖ Observ√°vel (Logs, M√©tricas, Alertas)
4. ‚úÖ Escal√°vel (Cache, Rate Limiting, Replica√ß√£o)
5. ‚úÖ Produ√ß√£o-ready (Tudo testado e validado)

**Recomenda√ß√£o**: Implementar Prioridade 1 e 2 antes de ir para produ√ß√£o.
