# Multi-Worker Scraping Guide

## ðŸš€ Overview

O sistema de scraper agora suporta **mÃºltiplos workers paralelos** usando Playwright para processar URLs de forma muito mais rÃ¡pida.

**BenefÃ­cios:**
- âœ… **2-3x mais rÃ¡pido** que scraping sequencial
- âœ… Melhor utilizaÃ§Ã£o de CPU/RAM
- âœ… ConfiguraÃ§Ã£o simples via JSON
- âœ… Retry automÃ¡tico com fallback
- âœ… Logging detalhado de progresso

---

## ðŸ“‹ Arquitetura

```
PlaywrightWorkerPool (IBrowserWorkerPool)
â”œâ”€â”€ Browser Instance (1)
â”‚   â””â”€â”€ Context (compartilhado = cookies/cache)
â”‚       â”œâ”€â”€ Page 1 (Worker 0)
â”‚       â”œâ”€â”€ Page 2 (Worker 1)
â”‚       â””â”€â”€ Page 3 (Worker 2)
â”‚
â”œâ”€â”€ asyncio.Queue
â”‚   â””â”€â”€ Distribuir URLs entre workers
â”‚
â””â”€â”€ asyncio.Semaphore
    â””â”€â”€ Limitar concorrÃªncia
```

**CaracterÃ­sticas:**
- MÃºltiplas **pÃ¡ginas no mesmo contexto** (evita re-login)
- **asyncio.Semaphore** para limite suave de concorrÃªncia
- **asyncio.Queue** para distribuiÃ§Ã£o de trabalho
- **retry automÃ¡tico** com exponential backoff
- **logging detalhado** por worker

---

## âš™ï¸ ConfiguraÃ§Ã£o

### 1. Via `scraper_config.json`

```json
{
  "concurrency": {
    "num_workers": 3,
    "enable_worker_pool": true,
    "max_urls_per_worker": 50,
    "worker_timeout_ms": 30000,
    "fallback_to_sequential": true
  }
}
```

**ParÃ¢metros:**
- `num_workers` (int): NÃºmero de pÃ¡ginas paralelas (recomendado: 2-5)
- `enable_worker_pool` (bool): Ativar/desativar worker pool
- `max_urls_per_worker` (int): MÃ¡x. URLs por worker antes de avisos
- `worker_timeout_ms` (int): Timeout para operaÃ§Ãµes de worker
- `fallback_to_sequential` (bool): Voltar para sequencial em caso de erro

### 2. Via Python

```python
from libs.scrapers.adapters import PlaywrightWorkerPool

# Criar pool
pool = PlaywrightWorkerPool(headless=True, timeout=30000)

# Inicializar com 3 workers
await pool.initialize(num_workers=3)

# Processar URLs
results = await pool.process_urls(
    urls=["https://example.com/1", "https://example.com/2"],
    worker_func=lambda url, worker_id: scrape(url, worker_id),
    show_progress=True
)

# Fechar
await pool.close()
```

---

## ðŸ“Š RecomendaÃ§Ãµes de Workers

| CenÃ¡rio | Workers | Notas |
|---------|---------|-------|
| Desenvolvimento | 1-2 | Menos memÃ³ria, mais fÃ¡cil debug |
| Scraping normal | 3-4 | Bom balance velocidade/estabilidade |
| Servidor poderoso | 5-8 | Mais paralelismo, maior throughput |
| Modo agressivo | 10+ | Alto risco de timeout/crash |

**FÃ³rmula aproximada:**
```
num_workers â‰ˆ (available_RAM_GB / 0.5) - 1
```

Cada worker Playwright consome ~500MB em mÃ©dia.

---

## ðŸ”§ Exemplo: IntegraÃ§Ã£o com Scraper Existente

### Antes (Sequencial)
```python
async def scrape_urls(urls):
    results = []
    for url in urls:
        try:
            doc = await scrape_single_url(url)
            results.append(doc)
        except Exception as e:
            logger.error(f"Error: {e}")
    return results

# â±ï¸ Lento: processa URLs uma por uma
result = asyncio.run(scrape_urls(urls))
```

### Depois (Com Workers)
```python
from libs.scrapers.adapters import PlaywrightWorkerPool

async def scrape_urls_with_workers(urls, num_workers=3):
    pool = PlaywrightWorkerPool(headless=True)
    
    try:
        await pool.initialize(num_workers=num_workers)
        
        results = await pool.process_urls(
            urls,
            worker_func=lambda url, wid: scrape_single_url(url, wid),
            show_progress=True
        )
        
        return results
        
    finally:
        await pool.close()

# âš¡ RÃ¡pido: processa URLs em paralelo
result = asyncio.run(scrape_urls_with_workers(urls, num_workers=3))
```

---

## ðŸ“ˆ Benchmark

### Exemplo: Scraping 100 URLs

```
Sequential (1 worker):
â”œâ”€â”€ Total Time: 250s
â”œâ”€â”€ Throughput: 0.4 URLs/s
â””â”€â”€ Status: â³ Lento

With 3 Workers:
â”œâ”€â”€ Total Time: 95s
â”œâ”€â”€ Throughput: 1.05 URLs/s
â””â”€â”€ Status: âœ… 2.6x mais rÃ¡pido!

With 5 Workers:
â”œâ”€â”€ Total Time: 65s
â”œâ”€â”€ Throughput: 1.54 URLs/s
â””â”€â”€ Status: âœ… 3.8x mais rÃ¡pido!
```

---

## ðŸ›¡ï¸ Tratamento de Erros

### Retry AutomÃ¡tico

```python
# Retry com atÃ© 3 tentativas
results = await pool.process_urls_with_retry(
    urls=urls,
    worker_func=scrape_func,
    max_retries=3,
    show_progress=True
)

# Resultado incluirÃ¡ todas as tentativas
successful = [r for r in results if r.success]
failed = [r for r in results if not r.success]
```

**EstratÃ©gia de Retry:**
1. Tentativa 1: Imediato
2. Tentativa 2: Aguarda 2s (exponential backoff)
3. Tentativa 3: Aguarda 4s

### Fallback para Sequencial

Se `fallback_to_sequential=true` em `scraper_config.json`:

```python
try:
    results = await pool.process_urls(urls, func)
except Exception as e:
    logger.warning("Worker pool failed, falling back to sequential")
    results = await process_sequentially(urls, func)
```

---

## ðŸ“Š Monitoramento

### Logs Detalhados

```
INFO: âœ… Initialized PlaywrightWorkerPool with 3 workers
INFO: Progress: 10/100 (10.0%) - Last: https://example.com/page1
DEBUG: âœ… Worker 0: https://example.com/page1... (1.23s)
DEBUG: âœ… Worker 1: https://example.com/page2... (1.45s)
DEBUG: âœ… Worker 2: https://example.com/page3... (0.98s)
...
INFO: âœ… Completed processing 100 URLs (98 successful)
```

### EstatÃ­sticas

```python
results = await pool.process_urls(urls, func)

# Calcular mÃ©tricas
total_time = sum(r.duration_seconds for r in results)
success_rate = sum(1 for r in results if r.success) / len(results)
avg_time = total_time / len(results)
throughput = len(results) / total_time

print(f"Success Rate: {success_rate * 100:.1f}%")
print(f"Avg Time/URL: {avg_time:.2f}s")
print(f"Throughput: {throughput:.2f} URLs/s")
```

---

## âš ï¸ Troubleshooting

### Problema: "Worker pool failed to initialize"

**Causa:** Falta de Playwright/Chromium instalado

```bash
# SoluÃ§Ã£o
playwright install chromium
```

### Problema: "Out of memory"

**Causa:** Muitos workers para a memÃ³ria disponÃ­vel

```python
# SoluÃ§Ã£o: Reduzir workers
await pool.initialize(num_workers=2)  # Ao invÃ©s de 5
```

### Problema: "Timeouts frequentes"

**Causa:** Worker timeout muito curto para URLs lentas

```json
{
  "concurrency": {
    "worker_timeout_ms": 60000
  }
}
```

### Problema: "Alguns URLs nÃ£o sÃ£o processados"

**Causa:** Worker travado ou exceÃ§Ã£o nÃ£o capturada

**SoluÃ§Ã£o:** Usar retry automÃ¡tico

```python
results = await pool.process_urls_with_retry(urls, func, max_retries=3)
```

---

## ðŸ” WorkerResult

Cada URL processada retorna um `WorkerResult`:

```python
@dataclass
class WorkerResult:
    url: str                      # URL processada
    success: bool                 # Se foi bem sucedido
    result: Any = None            # Resultado (Document, etc)
    error: Optional[str] = None   # Mensagem de erro (se falho)
    worker_id: int = -1           # ID do worker que processou
    duration_seconds: float = 0.0 # Tempo total
```

**Exemplo de anÃ¡lise:**

```python
results = await pool.process_urls(urls, func)

# Analisar por worker
for worker_id in range(pool.get_num_workers()):
    worker_results = [r for r in results if r.worker_id == worker_id]
    avg_time = sum(r.duration_seconds for r in worker_results) / len(worker_results)
    print(f"Worker {worker_id}: {len(worker_results)} URLs, avg {avg_time:.2f}s")

# Encontrar URLs lentas
slow_urls = [r for r in results if r.duration_seconds > 5.0]
for result in slow_urls:
    print(f"Slow: {result.url} ({result.duration_seconds:.2f}s)")

# Encontrar erros comuns
error_counts = {}
for result in results:
    if not result.success:
        error = result.error.split(":")[0]  # Tipo de erro
        error_counts[error] = error_counts.get(error, 0) + 1

for error, count in error_counts.items():
    print(f"{error}: {count} occurrences")
```

---

## ðŸŽ¯ Best Practices

### âœ… DO

```python
# âœ… Usar num_workers baseado em disponibilidade
num_workers = min(4, len(urls) // 10 + 1)

# âœ… Sempre fechar pool (use try/finally)
try:
    await pool.initialize(num_workers=3)
    results = await pool.process_urls(urls, func)
finally:
    await pool.close()

# âœ… Usar retry para URLs nÃ£o confiÃ¡veis
results = await pool.process_urls_with_retry(urls, func, max_retries=3)

# âœ… Monitorar sucesso
success_rate = sum(1 for r in results if r.success) / len(results)
if success_rate < 0.95:
    logger.warning(f"Low success rate: {success_rate * 100:.1f}%")
```

### âŒ DON'T

```python
# âŒ Usar muitos workers
await pool.initialize(num_workers=20)  # Muito!

# âŒ Esquecer de fechar
pool = PlaywrightWorkerPool()
await pool.initialize(3)
await pool.process_urls(urls, func)
# Pool nÃ£o foi fechado! ðŸ”´

# âŒ Ignorar timeouts
# Sem configurar timeout apropriado para URLs lentas

# âŒ Processar URLs nÃ£o validadas
# Validar URLs antes de enviar ao pool
```

---

## ðŸ“š ReferÃªncias

- [Playwright Python Docs](https://playwright.dev/python/)
- [asyncio Documentation](https://docs.python.org/3/library/asyncio.html)
- [IBrowserWorkerPool Interface](../../libs/scrapers/ports/browser_worker_pool.py)
- [PlaywrightWorkerPool Implementation](../../libs/scrapers/adapters/playwright_worker_pool.py)
- [Worker Pool Examples](../../examples/worker_pool_usage.py)
