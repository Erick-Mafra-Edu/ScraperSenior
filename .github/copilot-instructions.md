# Copilot Instructions - Senior Documentation Scraper

## Arquitetura do Projeto

Este projeto utiliza **Hexagonal Architecture (Ports & Adapters)** dentro de uma estrutura **monorepo**.

### Princ√≠pios Arquiteturais

1. **Hexagonal Architecture**: Core isolado de frameworks e detalhes t√©cnicos
2. **Dependency Inversion**: Core depende de abstra√ß√µes (ports), n√£o de implementa√ß√µes
3. **Clean Architecture**: Camadas com depend√™ncias unidirecionais
4. **SOLID Principles**: Aplicados em todas as camadas

---

## Estrutura do Projeto (Monorepo v2.0)

Este projeto segue uma arquitetura **monorepo** com separa√ß√£o clara de responsabilidades:

### üìÅ Diret√≥rios Principais

#### `apps/` - Aplica√ß√µes Execut√°veis
Aplica√ß√µes standalone que podem ser executadas diretamente:
- **`apps/scraper/`** - Scrapers principais (scraper_unificado.py, scraper_modular.py)
  - `config/` - Configura√ß√µes espec√≠ficas do scraper
- **`apps/mcp-server/`** - MCP Server para busca em documenta√ß√£o
- **`apps/zendesk/`** - Integra√ß√£o com Zendesk e Suporte Senior

**Regra**: Apps devem ser autocontidos e importar apenas de `libs/`.

#### `libs/` - Bibliotecas Compartilhadas
C√≥digo reutiliz√°vel seguindo **Hexagonal Architecture**:

**`libs/scrapers/`** - Core do sistema de scraping (Hexagonal Architecture):
```
libs/scrapers/
‚îú‚îÄ‚îÄ domain/              # Entidades e Value Objects (n√∫cleo)
‚îÇ   ‚îú‚îÄ‚îÄ document.py          # Document entity
‚îÇ   ‚îú‚îÄ‚îÄ scraping_result.py   # ScrapingResult value object
‚îÇ   ‚îî‚îÄ‚îÄ metadata.py          # DocumentMetadata value object
‚îÇ
‚îú‚îÄ‚îÄ ports/               # Interfaces (contratos)
‚îÇ   ‚îú‚îÄ‚îÄ document_scraper.py      # IDocumentScraper
‚îÇ   ‚îú‚îÄ‚îÄ document_repository.py   # IDocumentRepository
‚îÇ   ‚îú‚îÄ‚îÄ content_extractor.py     # IContentExtractor
‚îÇ   ‚îî‚îÄ‚îÄ url_resolver.py          # IUrlResolver
‚îÇ
‚îú‚îÄ‚îÄ use_cases/           # L√≥gica de neg√≥cio
‚îÇ   ‚îú‚îÄ‚îÄ scrape_documentation.py  # ScrapeDocumentation
‚îÇ   ‚îú‚îÄ‚îÄ extract_release_notes.py # ExtractReleaseNotes
‚îÇ   ‚îî‚îÄ‚îÄ index_documents.py       # IndexDocuments
‚îÇ
‚îî‚îÄ‚îÄ adapters/            # Implementa√ß√µes concretas
    ‚îú‚îÄ‚îÄ senior_doc_adapter.py    # SeniorDocAdapter
    ‚îú‚îÄ‚îÄ zendesk_adapter.py       # ZendeskAdapter
    ‚îú‚îÄ‚îÄ filesystem_repository.py # FileSystemRepository
    ‚îî‚îÄ‚îÄ playwright_extractor.py  # PlaywrightExtractor
```

**`libs/indexers/`** - Indexadores (JSONL local + Meilisearch)
**`libs/pipelines/`** - Data pipelines e transforma√ß√µes
**`libs/utils/`** - Fun√ß√µes utilit√°rias compartilhadas

**Regras**:
- Libs n√£o devem importar de `apps/`, apenas de outras `libs/`
- **Domain** n√£o depende de nada (n√∫cleo puro)
- **Ports** definem contratos (interfaces abstratas)
- **Use Cases** dependem apenas de **Domain** e **Ports**
- **Adapters** implementam **Ports** e podem depender de frameworks

#### `scripts/` - Utilit√°rios e Ferramentas
Scripts auxiliares organizados por categoria:
- **`scripts/analysis/`** - An√°lise de dados e estruturas
- **`scripts/indexing/`** - Indexa√ß√£o manual e reindexa√ß√£o
- **`scripts/fixes/`** - Debug e corre√ß√µes
- **`scripts/queries/`** - Consultas e verifica√ß√µes

**Regra**: Scripts podem importar de `apps/` e `libs/` conforme necess√°rio.

#### `data/` - Dados e Outputs
Dados separados do c√≥digo para facilitar backup/deploy:
- **`data/scraped/`** - Dados extra√≠dos
  - `estruturado/` - Docs estruturados por m√≥dulo (~1866 arquivos)
  - `unified/` - Docs unificados
  - `zendesk/` - Dados do Zendesk
- **`data/indexes/`** - √çndices JSONL para busca
- **`data/metadata/`** - Metadados e configura√ß√µes geradas

**Regra**: Nunca commitar dados grandes. Usar .gitignore apropriado.

#### `docs/` - Documenta√ß√£o
Documenta√ß√£o consolidada e organizada:
- **`docs/guides/`** - Guias de uso (Quick Start, MCP Server, etc.)
- **`docs/architecture/`** - Decis√µes t√©cnicas e arquitetura
- **`docs/api/`** - Documenta√ß√£o de API (futura)

**Regra**: Manter docs atualizados ao fazer mudan√ßas significativas.

#### `tests/` - Testes
Testes organizados por tipo:
- **`tests/unit/`** - Testes unit√°rios
- **`tests/integration/`** - Testes de integra√ß√£o
- **`tests/e2e/`** - Testes end-to-end
- **`tests/fixtures/`** - Dados de teste

**Regra**: Novos features devem incluir testes.

#### `infra/` - Infraestrutura
Configura√ß√µes de infraestrutura:
- **`infra/docker/`** - Dockerfiles e docker-compose
- **`infra/ci/`** - CI/CD pipelines

**Regra**: Testar mudan√ßas de Docker localmente antes de commitar.

---

## Hexagonal Architecture - Padr√µes e Conven√ß√µes

### Fluxo de Depend√™ncias

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    APPLICATION CORE                      ‚îÇ
‚îÇ                                                           ‚îÇ
‚îÇ  Domain (entities) ‚Üê Ports (interfaces) ‚Üê Use Cases     ‚îÇ
‚îÇ                                                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚ñ≤
                            ‚îÇ depends on
                            ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     ADAPTERS                             ‚îÇ
‚îÇ  (implementam ports, conhecem frameworks)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Regra de Ouro**: Depend√™ncias sempre apontam para dentro (core).

### Camadas da Arquitetura Hexagonal

#### 1. **Domain Layer** (`libs/scrapers/domain/`)
- **O que √©**: Entidades e Value Objects do neg√≥cio
- **Depende de**: Nada (n√∫cleo puro)
- **Responsabilidade**: Representar conceitos de neg√≥cio
- **Exemplo**: `Document`, `ScrapingResult`, `DocumentMetadata`

```python
# ‚úÖ CORRETO - Domain puro
from dataclasses import dataclass
from datetime import datetime

@dataclass
class Document:
    id: str
    title: str
    content: str
    # Sem depend√™ncias externas!
```

#### 2. **Ports Layer** (`libs/scrapers/ports/`)
- **O que √©**: Interfaces (contratos) que definem opera√ß√µes
- **Depende de**: Domain apenas
- **Responsabilidade**: Definir contratos entre core e mundo externo
- **Exemplo**: `IDocumentScraper`, `IDocumentRepository`

```python
# ‚úÖ CORRETO - Port (interface)
from abc import ABC, abstractmethod
from libs.scrapers.domain import Document

class IDocumentScraper(ABC):
    @abstractmethod
    async def scrape(self, url: str) -> Document:
        pass
```

#### 3. **Use Cases Layer** (`libs/scrapers/use_cases/`)
- **O que √©**: L√≥gica de neg√≥cio que orquestra opera√ß√µes
- **Depende de**: Domain + Ports (apenas interfaces)
- **Responsabilidade**: Coordenar fluxo de trabalho
- **Exemplo**: `ScrapeDocumentation`, `ExtractReleaseNotes`

```python
# ‚úÖ CORRETO - Use Case
from libs.scrapers.domain import Document, ScrapingResult
from libs.scrapers.ports import IDocumentScraper, IDocumentRepository

class ScrapeDocumentation:
    def __init__(
        self,
        scrapers: List[IDocumentScraper],  # Depende de interface!
        repository: IDocumentRepository,   # N√£o de implementa√ß√£o!
    ):
        self.scrapers = scrapers
        self.repository = repository
    
    async def execute(self, urls: List[str]) -> ScrapingResult:
        # L√≥gica de neg√≥cio aqui
        pass
```

#### 4. **Adapters Layer** (`libs/scrapers/adapters/`)
- **O que √©**: Implementa√ß√µes concretas dos ports
- **Depende de**: Ports (implementa) + Frameworks externos
- **Responsabilidade**: Conectar core com tecnologias espec√≠ficas
- **Exemplo**: `SeniorDocAdapter`, `PlaywrightExtractor`

```python
# ‚úÖ CORRETO - Adapter
from playwright.async_api import async_playwright
from libs.scrapers.ports import IDocumentScraper
from libs.scrapers.domain import Document

class SeniorDocAdapter(IDocumentScraper):
    """Implementa IDocumentScraper para Senior Docs"""
    
    async def scrape(self, url: str) -> Document:
        # Usa Playwright (framework externo)
        async with async_playwright() as p:
            browser = await p.chromium.launch()
            # ... scraping logic
            return Document(...)
```

### Criando Novos Componentes

#### ‚úÖ Criar Nova Entidade (Domain)

```python
# libs/scrapers/domain/nova_entidade.py
from dataclasses import dataclass

@dataclass
class MinhaEntidade:
    """Entidade de neg√≥cio pura - sem depend√™ncias externas"""
    id: str
    nome: str
    
    def metodo_de_negocio(self) -> str:
        # L√≥gica de neg√≥cio pura
        return self.nome.upper()
```

#### ‚úÖ Criar Novo Port (Interface)

```python
# libs/scrapers/ports/meu_port.py
from abc import ABC, abstractmethod
from typing import List
from libs.scrapers.domain import MinhaEntidade

class IMeuPort(ABC):
    """Define contrato para opera√ß√£o X"""
    
    @abstractmethod
    async def fazer_algo(self, param: str) -> MinhaEntidade:
        """Docstring explicando o contrato"""
        pass
```

#### ‚úÖ Criar Novo Use Case

```python
# libs/scrapers/use_cases/meu_use_case.py
from libs.scrapers.domain import MinhaEntidade
from libs.scrapers.ports import IMeuPort

class MeuUseCase:
    """Orquestra l√≥gica de neg√≥cio"""
    
    def __init__(self, port: IMeuPort):
        self.port = port  # Depende de interface!
    
    async def execute(self, param: str) -> MinhaEntidade:
        # L√≥gica de neg√≥cio orquestrando ports
        resultado = await self.port.fazer_algo(param)
        # Processar resultado...
        return resultado
```

#### ‚úÖ Criar Novo Adapter

```python
# libs/scrapers/adapters/meu_adapter.py
from libs.scrapers.ports import IMeuPort
from libs.scrapers.domain import MinhaEntidade

class MeuAdapter(IMeuPort):
    """Implementa√ß√£o concreta de IMeuPort"""
    
    async def fazer_algo(self, param: str) -> MinhaEntidade:
        # Implementa√ß√£o usando tecnologias espec√≠ficas
        # (banco de dados, APIs, scraping, etc.)
        return MinhaEntidade(id="1", nome=param)
```

### Dependency Injection Pattern

```python
# apps/scraper/main.py (Bootstrap)

# 1. Criar adapters (implementa√ß√µes)
scraper_adapter = SeniorDocAdapter(extractor=PlaywrightExtractor())
zendesk_adapter = ZendeskAdapter(extractor=PlaywrightExtractor())
repository = FileSystemRepository(output_dir="data/scraped/")

# 2. Injetar no use case
use_case = ScrapeDocumentation(
    scrapers=[scraper_adapter, zendesk_adapter],
    repository=repository
)

# 3. Executar
result = await use_case.execute(urls=["https://..."])
```

### Testes com Mocks

```python
# tests/unit/test_use_case.py
from unittest.mock import AsyncMock
from libs.scrapers.use_cases import ScrapeDocumentation

async def test_scrape_documentation():
    # Mock dos ports
    mock_scraper = AsyncMock(IDocumentScraper)
    mock_repository = AsyncMock(IDocumentRepository)
    
    # Use case test√°vel sem adapters reais!
    use_case = ScrapeDocumentation(
        scrapers=[mock_scraper],
        repository=mock_repository
    )
    
    result = await use_case.execute(["https://test.com"])
    
    # Assertions...
```

### Anti-Patterns (‚ùå Evitar)

```python
# ‚ùå ERRADO - Domain dependendo de framework
from playwright.async_api import Page

@dataclass
class Document:
    page: Page  # N√ÉO! Domain n√£o pode depender de Playwright

# ‚ùå ERRADO - Use Case dependendo de implementa√ß√£o
from libs.scrapers.adapters.senior_doc_adapter import SeniorDocAdapter

class ScrapeDocumentation:
    def __init__(self, scraper: SeniorDocAdapter):  # N√ÉO! Depende de implementa√ß√£o
        pass

# ‚ùå ERRADO - Adapter no domain
from libs.scrapers.adapters import FileSystemRepository

@dataclass
class Document:
    def save(self):
        repo = FileSystemRepository()  # N√ÉO! Domain n√£o chama adapters
        repo.save(self)
```

---

## Melhores Pr√°ticas

### 1. Imports (Hexagonal Architecture)

```python
# ‚úÖ CORRETO - Use Case importando apenas abstra√ß√µes
from libs.scrapers.domain import Document, ScrapingResult
from libs.scrapers.ports import IDocumentScraper, IDocumentRepository

class MeuUseCase:
    def __init__(
        self,
        scraper: IDocumentScraper,      # Interface, n√£o implementa√ß√£o
        repository: IDocumentRepository  # Interface, n√£o implementa√ß√£o
    ):
        pass

# ‚úÖ CORRETO - Adapter implementando interface
from libs.scrapers.ports import IDocumentScraper
from libs.scrapers.domain import Document
import external_framework  # OK em adapters

class MeuAdapter(IDocumentScraper):
    async def scrape(self, url: str) -> Document:
        # Pode usar frameworks externos aqui
        pass

# ‚ùå ERRADO - Use Case dependendo de adapter
from libs.scrapers.adapters.senior_doc_adapter import SeniorDocAdapter

class MeuUseCase:
    def __init__(self, scraper: SeniorDocAdapter):  # N√ÉO!
        pass
```

### 2. Imports Gerais
```python
# ‚úÖ CORRETO - Imports absolutos da nova estrutura
from apps.scraper.scraper_unificado import ScraperUnificado
from libs.indexers.index_local import LocalIndexer
from libs.utils.logger import setup_logger

# ‚ùå ERRADO - Imports antigos (pr√©-refatora√ß√£o)
from src.scraper_unificado import ScraperUnificado
from src.indexers.index_local import LocalIndexer
```

### 3. Paths de Arquivos
```python
# ‚úÖ CORRETO - Paths relativos √† nova estrutura
config_path = "apps/scraper/config/scraper_config.json"
data_path = "data/scraped/estruturado/"
index_path = "data/indexes/docs_indexacao.jsonl"

# ‚ùå ERRADO - Paths antigos
config_path = "scraper_config.json"
data_path = "docs_estruturado/"
index_path = "docs_indexacao.jsonl"
```

### 4. Cria√ß√£o de Novos M√≥dulos (Hexagonal)

**Domain Entity**:
```bash
# Criar nova entidade de neg√≥cio
touch libs/scrapers/domain/nova_entidade.py
# Adicionar em libs/scrapers/domain/__init__.py
```

**Port (Interface)**:
```bash
# Criar nova interface
touch libs/scrapers/ports/novo_port.py
# Adicionar em libs/scrapers/ports/__init__.py
```

**Use Case**:
```bash
# Criar novo caso de uso
touch libs/scrapers/use_cases/novo_use_case.py
# Adicionar em libs/scrapers/use_cases/__init__.py
```

**Adapter**:
```bash
# Criar nova implementa√ß√£o
touch libs/scrapers/adapters/novo_adapter.py
# N√£o adicionar em __init__.py (injetado via DI)
```

### 5. Cria√ß√£o de Novos M√≥dulos (Geral)

**App novo**:
```bash
mkdir -p apps/novo-app/config
touch apps/novo-app/__init__.py
touch apps/novo-app/main.py
touch apps/novo-app/config/config.json
```

**Lib nova**:
```bash
mkdir -p libs/nova-lib
touch libs/nova-lib/__init__.py
touch libs/nova-lib/module.py
```

**Script novo**:
```bash
# Identificar categoria: analysis, indexing, fixes, ou queries
touch scripts/analysis/novo_script.py
```

### 6. Documenta√ß√£o

Ao adicionar features ou fazer mudan√ßas:
1. Atualizar `CHANGELOG.md` com entrada datada
2. Criar/atualizar guia em `docs/guides/` se necess√°rio
3. Documentar breaking changes
4. Atualizar `README.md` se mudar interface p√∫blica

### 7. Testes (Hexagonal)

**Testes Unit√°rios (Domain & Use Cases)**:
```python
# tests/unit/domain/test_document.py
from libs.scrapers.domain import Document

def test_document_word_count():
    doc = Document(
        id="1", url="test", title="Test",
        content="Hello world", module="test"
    )
    assert doc.word_count() == 2

# tests/unit/use_cases/test_scrape_documentation.py
from unittest.mock import AsyncMock
from libs.scrapers.use_cases import ScrapeDocumentation

async def test_execute_with_mocked_scraper():
    mock_scraper = AsyncMock()
    mock_repository = AsyncMock()
    
    use_case = ScrapeDocumentation(
        scrapers=[mock_scraper],
        repository=mock_repository
    )
    # Test sem depend√™ncias reais!
```

**Testes de Integra√ß√£o (Adapters)**:
```python
# tests/integration/adapters/test_senior_doc_adapter.py
from libs.scrapers.adapters import SeniorDocAdapter

async def test_senior_doc_adapter_real_scraping():
    adapter = SeniorDocAdapter()
    doc = await adapter.scrape("https://real-url.com")
    assert doc.title
    await adapter.close()
```

### 8. Testes Gerais

```bash
# Executar todos os testes
pytest tests/

# Executar categoria espec√≠fica
pytest tests/integration/
pytest tests/unit/

# Executar arquivo espec√≠fico
pytest tests/integration/test_mcp_server.py
```

### 9. Docker

```bash
# Build e run local (de dentro de infra/docker/)
cd infra/docker
docker-compose up -d meilisearch
docker-compose up mcp-server

# Verificar logs
docker-compose logs -f mcp-server
```

### 10. Dados

- **Scraping**: Output vai para `data/scraped/`
- **Indexa√ß√£o**: √çndices v√£o para `data/indexes/`
- **Metadados**: JSON files v√£o para `data/metadata/`
- **Backups**: Usar `backups/` na raiz

---

## Refer√™ncias R√°pidas

### Comandos Principais

```bash
# Scraper
python apps/scraper/scraper_unificado.py

# MCP Server
python apps/mcp-server/mcp_server.py

# Indexa√ß√£o
python scripts/indexing/reindex_all_docs.py

# Testes
pytest tests/

# CI/CD
powershell infra/ci/ci_pipeline.ps1
```

### Estrutura de Imports

```
apps/
  ‚îî‚îÄ scraper/  ‚îÄ‚îê
  ‚îî‚îÄ mcp-server/ ‚îú‚îÄ‚îÄ> libs/
  ‚îî‚îÄ zendesk/   ‚îÄ‚îò      ‚îî‚îÄ scrapers/
                        ‚îî‚îÄ indexers/
scripts/              ‚îî‚îÄ utils/
  ‚îî‚îÄ pode importar de apps/ e libs/
```

### Arquivos de Configura√ß√£o

- `apps/scraper/config/scraper_config.json` - Config do scraper
- `apps/scraper/config/release_notes_config.json` - Config de release notes
- `apps/mcp-server/mcp_config.json` - Config do MCP server
- `infra/docker/docker-compose.yml` - Orquestra√ß√£o Docker

---

## Breaking Changes (v2.0)

Se estiver migrando c√≥digo antigo:

1. **Atualizar imports**: `src.*` ‚Üí `apps.*` ou `libs.*`
2. **Atualizar paths de config**: Mover configs para `apps/*/config/`
3. **Atualizar paths de dados**: Referenciar `data/` ao inv√©s da raiz
4. **Atualizar Docker volumes**: Apontar para novos paths

Ver `REFACTORING_NOTES.md` para detalhes completos da migra√ß√£o.

---

## Checklist para Novos Features (Hexagonal)

Ao adicionar um novo feature de scraping:

- [ ] **Domain**: Criar/atualizar entidades se necess√°rio
- [ ] **Port**: Definir interface se for nova opera√ß√£o
- [ ] **Use Case**: Implementar l√≥gica de neg√≥cio
- [ ] **Adapter**: Implementar conex√£o com tecnologia espec√≠fica
- [ ] **Tests**: Unit tests (use cases) + Integration tests (adapters)
- [ ] **Bootstrap**: Adicionar DI no main.py
- [ ] **Docs**: Atualizar documenta√ß√£o

## Exemplos R√°pidos

### Adicionar Novo Scraper (ex: Confluence)

```python
# 1. Criar Adapter
# libs/scrapers/adapters/confluence_adapter.py
from libs.scrapers.ports import IDocumentScraper
from libs.scrapers.domain import Document, DocumentSource, DocumentType

class ConfluenceAdapter(IDocumentScraper):
    async def scrape(self, url: str) -> Document:
        # Implementa√ß√£o espec√≠fica
        return Document(
            id=url,
            url=url,
            title="...",
            content="...",
            module="confluence",
            doc_type=DocumentType.TECHNICAL_DOC,
            source=DocumentSource.UNKNOWN  # Adicionar CONFLUENCE no enum
        )
    
    def supports_url(self, url: str) -> bool:
        return "confluence" in url.lower()

# 2. Registrar no Bootstrap
# apps/scraper/main.py
confluence = ConfluenceAdapter(extractor=PlaywrightExtractor())
use_case = ScrapeDocumentation(
    scrapers=[senior_adapter, zendesk_adapter, confluence],  # Adicionar
    repository=repository
)
```

### Adicionar Novo Reposit√≥rio (ex: MongoDB)

```python
# 1. Criar Adapter
# libs/scrapers/adapters/mongodb_repository.py
from libs.scrapers.ports import IDocumentRepository
from libs.scrapers.domain import Document

class MongoDBRepository(IDocumentRepository):
    def __init__(self, connection_string: str):
        # Setup MongoDB
        pass
    
    async def save(self, document: Document) -> None:
        # Salvar no MongoDB
        pass
    
    # Implementar outros m√©todos do IDocumentRepository...

# 2. Usar no Bootstrap
repository = MongoDBRepository("mongodb://localhost:27017/docs")
use_case = ScrapeDocumentation(scrapers, repository)
```

## Suporte

- **Documenta√ß√£o**: Ver `docs/` para guias completos
- **Changelog**: Ver `CHANGELOG.md` para hist√≥rico
- **Issues**: Documentar problemas e solu√ß√µes no projeto
- **Refactoring Notes**: Ver `REFACTORING_NOTES.md` para contexto da migra√ß√£o
- **Hexagonal Plan**: Ver session workspace `hexagonal-plan.md` para detalhes da arquitetura