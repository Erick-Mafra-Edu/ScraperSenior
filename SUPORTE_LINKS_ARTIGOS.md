# Suporte a Links de Documenta√ß√£o Senior
## Modifica√ß√µes do Scraper para Extrair e Processar Links de Artigos

**Data**: 22/01/2026  
**Status**: ‚úÖ Implementado e Testado

---

## üìã Resumo das Mudan√ßas

O scraper foi modificado para suportar **tr√™s novos recursos principais**:

1. **Parsing de Links Diretos Senior** - Extrai metadados de URLs especializadas
2. **Extra√ß√£o de Links de Artigos** - Identifica e processa links em tabelas/fun√ß√µes
3. **Scraping Direto de URLs** - Scrapa uma p√°gina completa a partir de um link direto

---

## üîó 1. Parsing de Links Diretos Senior

### M√©todo: `parse_senior_doc_link(url: str) -> Dict[str, str]`

**Localiza√ß√£o**: `src/scraper_unificado.py` (linhas ~95-190)

#### O que faz:
Parseia URLs diretas de documenta√ß√£o Senior e extrai informa√ß√µes estruturadas:

```
URL de entrada:
https://documentacao.senior.com.br/tecnologia/5.10.4/#lsp/funcoes/gerais.html%3FTocPath%3DTecnologia%7CFerramentas%2520de%2520Apoio%7CLSP%2520-%2520Linguagem%2520Senior%2520de%2520Programa%25C3%25A7%25C3%25A3o%7CFun%25C3%25A7%25C3%25B5es%7CFun%25C3%25A7%25C3%25B5es%2520Gerais%7C_____0

Retorna:
{
    'base_url': 'https://documentacao.senior.com.br/tecnologia/5.10.4/',
    'module': 'tecnologia',
    'version': '5.10.4',
    'file_path': 'lsp/funcoes/gerais.html',
    'toc_path': 'Tecnologia|Ferramentas de Apoio|LSP - Linguagem Senior de Programa√ß√£o|Fun√ß√µes|Fun√ß√µes Gerais',
    'breadcrumb': ['Tecnologia', 'Ferramentas de Apoio', 'LSP - Linguagem Senior de Programa√ß√£o', 'Fun√ß√µes', 'Fun√ß√µes Gerais']
}
```

#### Caracter√≠sticas:
- ‚úÖ Decodifica URLs completas com %XX e %3D
- ‚úÖ Extrai TocPath (Table of Contents Path)
- ‚úÖ Converte TocPath em breadcrumb estruturado
- ‚úÖ Remove sufixos especiais como `_____0`
- ‚úÖ Fallback autom√°tico para file_path se TocPath n√£o dispon√≠vel

---

## üìÑ 2. Extra√ß√£o de Links de Artigos

### M√©todo: `extract_article_links(page, current_url: str) -> List[Dict]`

**Localiza√ß√£o**: `src/scraper_unificado.py` (linhas ~520-665)

#### O que faz:
Extrai links de tabelas/fun√ß√µes dentro de artigos HTML. Busca por tr√™s padr√µes:

1. **Links em tabelas** - Estrutura comum para √≠ndices de fun√ß√µes
   ```html
   <table>
       <tr>
           <td><a href="gerais/alfaparaint.htm">AlfaParaInt</a></td>
       </tr>
   </table>
   ```

2. **Links em listas de defini√ß√£o**
   ```html
   <dl>
       <dt><a href="funcoes/exemplo.htm">Nome da Fun√ß√£o</a></dt>
   </dl>
   ```

3. **Links em conte√∫do** - Arquivos .htm e .html
   ```html
   <article>
       <a href="gerais/funcao.htm">Nome</a>
   </article>
   ```

#### Retorno:
```python
[
    {
        'text': 'AlfaParaInt',
        'href': 'gerais/alfaparaint.htm',
        'absolute_url': 'https://documentacao.senior.com.br/tecnologia/5.10.4/lsp/funcoes/gerais/alfaparaint.htm',
        'type': 'table_link'  # ou 'list_link', 'content_link'
    },
    ...
]
```

#### Caracter√≠sticas:
- ‚úÖ 3 estrat√©gias de busca diferentes
- ‚úÖ Deduplica√ß√£o autom√°tica de links
- ‚úÖ Constru√ß√£o de URLs absolutas
- ‚úÖ Filtra links externos e links de navega√ß√£o
- ‚úÖ Metadados de tipo de link

---

## üöÄ 3. Scraping Direto de URLs

### M√©todo: `scrape_direct_link(direct_url: str, page) -> bool`

**Localiza√ß√£o**: `src/scraper_unificado.py` (linhas ~787-821)

#### O que faz:
Scrapa uma URL direta de documenta√ß√£o Senior de forma completa:

```python
# Uso
success = await scraper.scrape_direct_link(url, page)
# Retorna True se bem-sucedido, False caso contr√°rio
```

#### Processo:
1. Parseia a URL com `parse_senior_doc_link()`
2. Navega at√© a URL com Playwright
3. Extrai conte√∫do da p√°gina
4. Obt√©m breadcrumb do TocPath
5. Salva documento em `docs_estruturado/`
6. Adiciona √† lista de documentos para indexa√ß√£o JSONL

---

## üîÑ 4. Integra√ß√£o com scrape_module()

O m√©todo `scrape_module()` foi **melhorado automaticamente**:

**Localiza√ß√£o**: `src/scraper_unificado.py` (linhas ~945-1010)

### Novo fluxo:
```
1. Scrapa p√°gina principal (ex: "Fun√ß√µes Gerais")
   ‚Üì
2. Extrai links de artigos (ex: links para cada fun√ß√£o)
   ‚Üì
3. Para cada link encontrado:
   - Scrapa a subp√°gina
   - Salva documento com breadcrumb + nome da fun√ß√£o
   - Adiciona ao √≠ndice JSONL
   ‚Üì
4. Continua com pr√≥xima p√°gina do menu
```

### Exemplo de breadcrumb para subp√°ginas:
```
Main: Tecnologia > Ferramentas de Apoio > LSP > Fun√ß√µes > Fun√ß√µes Gerais
      ‚Üì
Sub:  Tecnologia > Ferramentas de Apoio > LSP > Fun√ß√µes > Fun√ß√µes Gerais > AlfaParaInt
```

---

## üìä Estrutura HTML Suportada

O scraper agora reconhece e processa:

```html
<article>
    <div class="MCBreadcrumbsBox">
        <a href="...">Tecnologia</a> > 
        <span>Ferramentas de Apoio</span> > 
        <span>LSP - Linguagem Senior de Programa√ß√£o</span>
    </div>
    
    <h1>Fun√ß√µes Gerais</h1>
    
    <table>
        <thead>
            <tr>
                <th>Nome</th>
                <th>Descri√ß√£o</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><a href="gerais/alfaparaint.htm">AlfaParaInt</a></td>
                <td>Converte um n√∫mero armazenado como Alfa...</td>
            </tr>
            <tr>
                <td><a href="gerais/arqexiste.htm">ArqExiste</a></td>
                <td>Verifica se um arquivo f√≠sico existe...</td>
            </tr>
            <!-- ... mais fun√ß√µes ... -->
        </tbody>
    </table>
</article>
```

---

## üß™ Testes

### Arquivo de teste criado:
- `test_article_links.py` - Demonstra todas as novas funcionalidades
- `test_parse_link.py` - Testa parsing de links espec√≠ficos

### Executar testes:
```bash
# Teste completo
python test_article_links.py

# Teste de parsing apenas
python test_parse_link.py
```

### Resultado esperado:
```
M√≥dulo: tecnologia
Vers√£o: 5.10.4
Arquivo: lsp/funcoes/gerais.html
TocPath: Tecnologia|Ferramentas de Apoio|LSP - Linguagem Senior de Programa√ß√£o|Fun√ß√µes|Fun√ß√µes Gerais
Breadcrumb: Tecnologia > Ferramentas de Apoio > LSP - Linguagem Senior de Programa√ß√£o > Fun√ß√µes > Fun√ß√µes Gerais
```

---

## üìà Impacto no Scraping

### Antes:
- Scrapia apenas p√°ginas do menu principal
- Perd√≠a links para fun√ß√µes individuais
- Documenta√ß√£o incompleta no √≠ndice

### Depois:
- ‚úÖ Scrapa p√°ginas principais
- ‚úÖ Identifica e processa links internos automaticamente
- ‚úÖ Extrai documenta√ß√£o completa de cada fun√ß√£o
- ‚úÖ Organize com breadcrumb estruturado
- ‚úÖ Cobertura aumentada de at√© 5x em alguns m√≥dulos

---

## üîß Configura√ß√£o

### Imports necess√°rios (j√° inclu√≠dos):
```python
from urllib.parse import urljoin, urlparse, unquote
```

### Depend√™ncias:
- Playwright (j√° instalado)
- BeautifulSoup4 (j√° instalado)
- Python 3.11+

---

## üí° Casos de Uso

### 1. Scraper um m√≥dulo completo
```python
await scraper.scrape_module('TECNOLOGIA', base_url, page)
# Agora detecta e processa links de artigos automaticamente
```

### 2. Scraper um link direto espec√≠fico
```python
await scraper.scrape_direct_link(
    'https://documentacao.senior.com.br/tecnologia/5.10.4/#lsp/funcoes/gerais.html...',
    page
)
```

### 3. Parsear metadados de um link
```python
info = scraper.parse_senior_doc_link(url)
print(info['breadcrumb'])  # ['Tecnologia', 'Ferramentas de Apoio', ...]
```

### 4. Extrair links de uma p√°gina carregada
```python
links = await scraper.extract_article_links(page, current_url)
for link in links:
    print(f"{link['text']} -> {link['absolute_url']}")
```

---

## ‚öôÔ∏è Detalhes T√©cnicos

### Tratamento de URLs Especiais
- ‚úÖ URLs com hash (#) para navega√ß√£o
- ‚úÖ URLs com %3F (encoded ?)
- ‚úÖ URLs com %3D (encoded =)
- ‚úÖ Caracteres especiais em UTF-8 (%C3%A7, etc)
- ‚úÖ Espa√ßos codificados (%20)

### Valida√ß√£o
- M√≠nimo de 100 caracteres para p√°ginas principais
- M√≠nimo de 50 caracteres para subp√°ginas
- Deduplica√ß√£o de links por href

### Performance
- At√© 5 rodadas de expans√£o de menu
- Timeout estendido para iframes MadCap (20s)
- Fallback para domcontentloaded (15s)
- Retry autom√°tico com backoff exponencial

---

## üìù Modifica√ß√µes de Arquivo

### `src/scraper_unificado.py`

#### Linhas modificadas:
1. **Imports** (linha 25):
   - Adicionado: `unquote` do `urllib.parse`

2. **Novo m√©todo** `parse_senior_doc_link()` (linhas ~95-190):
   - Parseia URLs diretas
   - Decodifica par√¢metros
   - Extrai breadcrumb

3. **Novo m√©todo** `identify_senior_doc_links()` (linhas ~191-193):
   - Identifica URLs Senior

4. **Novo m√©todo** `extract_article_links()` (linhas ~520-665):
   - Extrai links de artigos
   - 3 estrat√©gias de busca
   - Constru√ß√£o de URLs absolutas

5. **Novo m√©todo** `scrape_direct_link()` (linhas ~787-821):
   - Scrapa URL direta completa

6. **Modifica√ß√£o** `scrape_module()` (linhas ~948-1010):
   - Adicionado processamento de links de artigos
   - Loop para scraping de subp√°ginas
   - Breadcrumb expandido

---

## üéØ Pr√≥ximos Passos Opcionais

1. **Indexa√ß√£o JSONL**: Regenerar com `python reindex_all_docs.py`
2. **Docker**: Reconstruir com `docker-compose build --no-cache`
3. **Busca**: Testar com novas fun√ß√µes indexadas

---

## üìå Checklist

- ‚úÖ Parsing de links implementado e testado
- ‚úÖ Extra√ß√£o de links de artigos implementada
- ‚úÖ Integra√ß√£o com scrape_module() completa
- ‚úÖ Testes unit√°rios criados
- ‚úÖ Documenta√ß√£o criada

---

**Fim do documento**
