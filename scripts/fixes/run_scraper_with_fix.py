#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Executa o scraper com as novas altera√ß√µes
Testa um m√≥dulo completo para validar que t√≠tulos est√£o sendo capturados
"""

import asyncio
import json
from pathlib import Path
from playwright.async_api import async_playwright
from src.scraper_unificado import SeniorDocScraper


async def execute_scraper_with_fix():
    """Executa scraper com altera√ß√µes"""
    
    print("\n" + "="*80)
    print("[EXECU√á√ÉO] Scraper com Corre√ß√£o de T√≠tulos")
    print("="*80 + "\n")
    
    # Carregar m√≥dulos
    modulos_file = Path("modulos_descobertos.json")
    with open(modulos_file) as f:
        modulos = json.load(f)
    
    scraper = SeniorDocScraper()
    
    # Testar m√≥dulo GESTAO DE PESSOAS HCM (maior, melhor para teste)
    module_name = "GESTAO DE PESSOAS HCM"
    base_url = modulos[module_name]['url']
    
    print(f"üîÑ Scraping do m√≥dulo: {module_name}")
    print(f"üìç URL base: {base_url}")
    print(f"‚è±Ô∏è  Iniciando...\n")
    
    try:
        # Usar Playwright para criar a p√°gina
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page(viewport={"width": 1920, "height": 1080})
            
            try:
                await scraper.scrape_module(module_name, base_url, page)
                
                docs = scraper.documents  # Pega os documentos coletados
                
                print(f"\n‚úÖ SCRAPING CONCLU√çDO!")
                print(f"üìä Total de documentos: {len(docs)}")
                
                # An√°lise de t√≠tulos
                docs_with_title = sum(1 for doc in docs if doc.get('title', '').strip())
                docs_without_title = len(docs) - docs_with_title
                
                print(f"\nüìù AN√ÅLISE DE T√çTULOS:")
                print(f"   ‚úÖ Com t√≠tulo: {docs_with_title}/{len(docs)} ({100*docs_with_title/len(docs):.1f}%)")
                print(f"   ‚ùå Sem t√≠tulo: {docs_without_title}")
                
                # Mostrar estat√≠sticas de conte√∫do
                print(f"\nüìà CONTE√öDO CAPTURADO:")
                
                total_chars = sum(len(doc.get('text_content', '')) for doc in docs)
                avg_chars = total_chars // len(docs) if docs else 0
                
                print(f"   Total de caracteres: {total_chars:,}")
                print(f"   M√©dia por documento: {avg_chars:,}")
                
                # Headers
                total_headers = sum(len(doc.get('headers', [])) for doc in docs)
                print(f"   Total de headers: {total_headers}")
                
                # Links
                total_links = sum(len(doc.get('links', [])) for doc in docs)
                print(f"   Total de links: {total_links}")
                
                # Listar primeiros 10 documentos com t√≠tulos
                print(f"\nüìÑ PRIMEIROS 10 DOCUMENTOS:")
                print(f"{'#':<3} {'T√≠tulo':<50} {'Chars':<8} {'Headers':<8}")
                print("-" * 75)
                
                for idx, doc in enumerate(docs[:10], 1):
                    title = doc.get('title', 'SEM T√çTULO')[:48]
                    chars = len(doc.get('text_content', ''))
                    headers = len(doc.get('headers', []))
                    print(f"{idx:<3} {title:<50} {chars:<8} {headers:<8}")
                
                # Buscar documento mais longo
                if docs:
                    longest = max(docs, key=lambda d: len(d.get('text_content', '')))
                    print(f"\nüèÜ MAIOR DOCUMENTO:")
                    print(f"   T√≠tulo: {longest.get('title', 'SEM T√çTULO')[:70]}")
                    print(f"   Caracteres: {len(longest.get('text_content', ''))}")
                    print(f"   URL: {longest.get('url', '')[:70]}")
                
                # Listar t√≠tulos √∫nicos (amostra)
                unique_titles = set(doc.get('title', '') for doc in docs)
                print(f"\nüè∑Ô∏è  T√çTULOS √öNICOS: {len(unique_titles)}")
                print("   Amostra de t√≠tulos:")
                for title in list(unique_titles)[:5]:
                    if title.strip():
                        print(f"     ‚úì {title[:60]}")
                
            finally:
                await browser.close()
        
    except Exception as e:
        print(f"\n‚ùå ERRO: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "="*80)
    print("[‚úì] EXECU√á√ÉO CONCLU√çDA")
    print("="*80 + "\n")


if __name__ == "__main__":
    asyncio.run(execute_scraper_with_fix())
